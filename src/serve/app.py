import argparse
from threading import Thread
import gradio as gr
from PIL import Image
from src.utils import load_pretrained_model, get_model_name_from_path, disable_torch_init
from transformers import TextIteratorStreamer
from functools import partial
import warnings
from qwen_vl_utils import process_vision_info
import gradio as gr

warnings.filterwarnings("ignore")


def is_video_file(filename):
    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg']
    return any(filename.lower().endswith(ext) for ext in video_extensions)


def bot_streaming(message, history, generation_args):
    """å¤šè½®å¯¹è¯ + å¤šæ¨¡æ€æ”¯æŒï¼ˆä¸ä½ çš„ç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰"""
    images, videos = [], []
    if message.get("files"):
        for file_item in message["files"]:
            file_path = file_item["path"] if isinstance(file_item, dict) else file_item
            if is_video_file(file_path):
                videos.append(file_path)
            else:
                images.append(file_path)

    conversation = []
    conversation.append({
        "role": "system",
        "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·è®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹å¹¶è¿ç»­å›ç­”ã€‚"}]
    })

    for user_turn, assistant_turn in history:
        if user_turn is None and assistant_turn is None:
            continue

        user_content = []

        if isinstance(user_turn, tuple):
            file_paths = []
            user_text = ""
            if len(user_turn) >= 1:
                f0 = user_turn[0]
                if f0:
                    file_paths = f0 if isinstance(f0, list) else [f0]
            if len(user_turn) >= 2 and user_turn[1] is not None:
                user_text = user_turn[1]

            for file_path in file_paths:
                if is_video_file(file_path):
                    user_content.append({"type": "video", "video": file_path, "fps": 1.0})
                else:
                    user_content.append({"type": "image", "image": file_path})

            if user_text:
                user_content.append({"type": "text", "text": user_text})

        elif isinstance(user_turn, str):
            user_content.append({"type": "text", "text": user_turn})

        if user_content:
            conversation.append({"role": "user", "content": user_content})

        if assistant_turn:
            conversation.append({
                "role": "assistant",
                "content": [{"type": "text", "text": assistant_turn}]
            })

    user_content = []
    for image in images:
        user_content.append({"type": "image", "image": image})
    for video in videos:
        user_content.append({"type": "video", "video": video, "fps": 1.0})
    if message.get('text'):
        user_content.append({"type": "text", "text": message['text']})
    if user_content:
        conversation.append({"role": "user", "content": user_content})

    conversation = conversation[-40:]

    prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(conversation)

    inputs = processor(
        text=[prompt],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to(device)

    streamer = TextIteratorStreamer(
        processor.tokenizer,
        skip_special_tokens=True,
        skip_prompt=True,
        clean_up_tokenization_spaces=False,
    )

    generation_kwargs = dict(
        **inputs,
        streamer=streamer,
        eos_token_id=processor.tokenizer.eos_token_id,
        max_new_tokens=generation_args.get("max_new_tokens", 1024),
        do_sample=True if generation_args.get("temperature", 0) > 0 else False,
        temperature=generation_args.get("temperature", 0) if generation_args.get("temperature", 0) > 0 else None,
        repetition_penalty=generation_args.get("repetition_penalty", 1.0),
    )

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    buffer = ""
    for new_text in streamer:
        buffer += new_text
        yield buffer


def main(args):
    global processor, model, device

    device = args.device
    disable_torch_init()

    use_flash_attn = True
    model_name = get_model_name_from_path(args.model_path)
    if args.disable_flash_attention:
        use_flash_attn = False

    processor, model = load_pretrained_model(
        model_base=args.model_base,
        model_path=args.model_path,
        device_map=args.device,
        model_name=model_name,
        load_4bit=args.load_4bit,
        load_8bit=args.load_8bit,
        device=args.device,
        use_flash_attn=use_flash_attn
    )

    generation_args = {
        "max_new_tokens": args.max_new_tokens,
        "temperature": args.temperature,
        "do_sample": True if args.temperature > 0 else False,
        "repetition_penalty": args.repetition_penalty,
    }

    bot_streaming_with_args = partial(bot_streaming, generation_args=generation_args)

    # âœ¨ æ”¹é€ åçš„ UI âœ¨
    with gr.Blocks(
        title="å¿åŸŸåœŸåœ°åˆ©ç”¨æ™ºèƒ½é—®ç­”å¹³å°",
        theme=gr.themes.Soft(primary_hue="green", secondary_hue="blue"),
        css=".gradio-container {background: #f7fafc;}"
    ) as demo:

        # é¡¶éƒ¨æ ‡é¢˜åŒº
        with gr.Column(scale=1, elem_id="app_header"):
            gr.Markdown("""
            <div style="text-align: center; margin-bottom: 20px;">
                <h1 style="color: #1f2937; font-size: 2.5em; font-weight: 600;">ğŸ›°ï¸ å¿åŸŸåœŸåœ°åˆ©ç”¨æ™ºèƒ½é—®ç­”å¹³å°</h1>
                <p style="color: #4b5563; font-size: 1.1em;">
                    é¢å‘ä¸­è¥¿éƒ¨å¿åŸŸè‡ªç„¶èµ„æºç®¡ç†çš„åˆ†é’Ÿçº§åŠ¨æ€ç›‘æµ‹ä¸å†³ç­–æ”¯æŒç³»ç»Ÿ
                </p>
                <p style="color: #6b7280; font-size: 0.9em; margin-top: 8px;">
                    åŸºäºä½åˆ†è¾¨ç‡å«æ˜Ÿå½±åƒå’Œå¤§æ¨¡å‹é—®ç­”æŠ€æœ¯ï¼Œè§£å†³â€œæ•°æ®éš¾è·å–ã€åˆ†æé—¨æ§›é«˜ã€ç›‘æµ‹ä¸åŠæ—¶â€ç­‰ç—›ç‚¹ï¼Œè®©éä¸“ä¸šäººå‘˜ä¹Ÿèƒ½é€šè¿‡è‡ªç„¶è¯­è¨€æé—®å¿«é€Ÿè·å–åœŸåœ°åŠ¨æ€ä¿¡æ¯ã€‚
                </p>
            </div>
            """)

        # åŠŸèƒ½ä»·å€¼å¡ç‰‡
        with gr.Row(equal_height=True, variant="panel", elem_id="feature_cards"):
            with gr.Column(scale=1, min_width=300):
                gr.Markdown("""
                <div style="padding: 15px; border-radius: 12px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); text-align: center;">
                    <h3 style="color: #16a34a;">ğŸ“Š æ•°æ®é€‚é…æ€§å¼º</h3>
                    <p style="color: #374151;">æ”¯æŒå¤šæºä½åˆ†è¾¨ç‡å«æ˜Ÿå½±åƒï¼Œè‡ªåŠ¨åŒ¹é…å¿åŸŸå°ºåº¦æ•°æ®</p>
                </div>
                """)
            with gr.Column(scale=1, min_width=300):
                gr.Markdown("""
                <div style="padding: 15px; border-radius: 12px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); text-align: center;">
                    <h3 style="color: #2563eb;">ğŸ’¡ é—®ç­”è´´è¿‘ä¸šåŠ¡</h3>
                    <p style="color: #374151;">ç”¨è‡ªç„¶è¯­è¨€æé—®å³å¯ï¼Œå¦‚â€œæ–°å¢è€•åœ°é¢ç§¯â€â€œç–‘ä¼¼è¿è§„å»ºè®¾â€</p>
                </div>
                """)
            with gr.Column(scale=1, min_width=300):
                gr.Markdown("""
                <div style="padding: 15px; border-radius: 12px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); text-align: center;">
                    <h3 style="color: #f97316;">âš¡ åˆ†é’Ÿçº§å“åº”</h3>
                    <p style="color: #374151;">æ¨¡å‹ç”Ÿæˆæ—¶é—´ < 2 åˆ†é’Ÿï¼ŒåŠ©åŠ›å®¡æ‰¹ã€æ‰§æ³•ä¸æŠ¥è¡¨å†³ç­–</p>
                </div>
                """)

        # ä¸»åŠŸèƒ½äº¤äº’åŒº
        with gr.Column(elem_id="main_chat_interface"):
            gr.Markdown("<h2 style='text-align: center; margin-top: 25px; margin-bottom: 20px; color: #1f2937;'>ğŸ” æ™ºèƒ½é—®ç­”äº¤äº’é¢æ¿</h2>")

            chatbot = gr.Chatbot(
                height=550,
                label="æ™ºèƒ½å¯¹è¯è®°å½•",
                bubble_full_width=False,
                avatar_images=(None, "https://img.alicdn.com/imgextra/i3/O1CN01eJSPaL1UXrmpANQjB_!!6000000002540-2-tps-128-128.png") # æ·»åŠ ä¸€ä¸ªAIå¤´åƒ
            )
            chat_input = gr.MultimodalTextbox(
                interactive=True,
                file_types=["image", "video"],
                placeholder="è¯·è¾“å…¥é—®é¢˜æˆ–ä¸Šä¼ å½±åƒæ–‡ä»¶ï¼ˆå¦‚ï¼š'2024å¹´ä¸ŠåŠå¹´è€•åœ°é¢ç§¯å˜åŒ–æ˜¯å¤šå°‘ï¼Ÿ'ï¼‰",
                show_label=False,
                file_count="multiple"
            )

            gr.ChatInterface(
                fn=bot_streaming_with_args,
                title=None, # éšè—é»˜è®¤æ ‡é¢˜
                stop_btn="â¹ï¸ åœæ­¢ç”Ÿæˆ",
                chatbot=chatbot,
                textbox=chat_input,
                examples=[
                    "2025 å¹´æ–°å¢è€•åœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ",
                    "æœ€è¿‘ä¸€ä¸ªå­£åº¦æ˜¯å¦å­˜åœ¨è¿è§„å»ºè®¾ï¼Ÿ",
                    "2024 ä¸ 2025 å¹´åœŸåœ°åˆ©ç”¨ç»“æ„æœ‰ä½•å˜åŒ–ï¼Ÿ"
                ],
            )

        # å¿«é€Ÿæé—®æ¨è
        with gr.Accordion("ğŸ“Œ å¿«é€Ÿæé—®ç¤ºä¾‹", open=False):
            gr.Markdown("""
            - ğŸ“ˆ æœ¬å­£åº¦è€•åœ°é¢ç§¯å˜åŒ–æƒ…å†µï¼Ÿ
            - ğŸ—ï¸ ç–‘ä¼¼è¿è§„å»ºè®¾çš„åˆ†å¸ƒåŒºåŸŸï¼Ÿ
            - ğŸŒ¿ ä¿æŠ¤åŒºè¾¹ç•Œæ˜¯å¦è¢«å ç”¨ï¼Ÿ
            - ğŸ—ºï¸ è¿‘ä¸€å¹´åœŸåœ°åˆ©ç”¨ç»“æ„è¶‹åŠ¿ï¼Ÿ
            """)

    demo.queue(api_open=False)
    demo.launch(show_api=False, share=False, server_name='0.0.0.0')


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, default=None)
    parser.add_argument("--model-base", type=str, default="Qwen/Qwen2.5-VL-3B-Instruct")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--load-8bit", action="store_true")
    parser.add_argument("--load-4bit", action="store_true")
    parser.add_argument("--disable_flash_attention", action="store_true")
    parser.add_argument("--temperature", type=float, default=0)
    parser.add_argument("--repetition-penalty", type=float, default=1.0)
    parser.add_argument("--max-new-tokens", type=int, default=1024)
    parser.add_argument("--debug", action="store_true")
    args = parser.parse_args()
    main(args)
